% This file was created with JabRef 2.9.2.
% Encoding: UTF8

@INPROCEEDINGS{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@INPROCEEDINGS{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}


@INPROCEEDINGS{vaswani-etal-2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@INPROCEEDINGS{
Lan2020ALBERT,
title={ALBERT: A Lite {BERT} for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@ARTICLE{Wu2016GooglesNM,
    title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    author={Yonghui Wu and Mike Schuster and Z. Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason R. Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Gregory S. Corrado and Macduff Hughes and Jeffrey Dean},
    journal={ArXiv},
    year={2016},
    volume={abs/1609.08144}
}

@ARTICLE{johnson-etal-2017-googles,
    title = "{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1024",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
}

@ARTICLE{Church2020May,
	author = {Church, Kenneth Ward},
	title = {{Emerging trends: Subwords, seriously?}},
	journal = {Nat. Lang. Eng.},
	volume = {26},
	number = {3},
	pages = {375--382},
	year = {2020},
	month = {May},
	issn = {1351-3249},
	publisher = {Cambridge University Press},
	doi = {10.1017/S1351324920000145}
}

@ARTICLE{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@INPROCEEDINGS{klein-tsarfaty-2020-getting,
    title = "Getting the {\#}{\#}life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?",
    author = "Klein, Stav  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sigmorphon-1.24",
    doi = "10.18653/v1/2020.sigmorphon-1.24",
    pages = "204--209",
    abstract = "This work investigates the most basic units that underlie contextualized word embeddings, such as BERT {---} the so-called word pieces. In Morphologically-Rich Languages (MRLs) which exhibit morphological fusion and non-concatenative morphology, the different units of meaning within a word may be fused, intertwined, and cannot be separated linearly. Therefore, when using word-pieces in MRLs, we must consider that: (1) a linear segmentation into sub-word units might not capture the full morphological complexity of words; and (2) representations that leave morphological knowledge on sub-word units inaccessible might negatively affect performance. Here we empirically examine the capacity of word-pieces to capture morphology by investigating the task of multi-tagging in Modern Hebrew, as a proxy to evaluate the underlying segmentation. Our results show that, while models trained to predict multi-tags for complete words outperform models tuned to predict the distinct tags of WPs, we can improve the WPs tag prediction by purposefully constraining the word-pieces to reflect their internal functions. We suggest that linguistically-informed word-pieces schemes, that make the morphological structure explicit, might boost performance for MRLs.",
}

@INPROCEEDINGS{creutz-lagus-2002-unsupervised,
    title = "Unsupervised Discovery of Morphemes",
    author = "Creutz, Mathias  and
      Lagus, Krista",
    booktitle = "Proceedings of the {ACL}-02 Workshop on Morphological and Phonological Learning",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-0603",
    doi = "10.3115/1118647.1118650",
    pages = "21--30",
}

@ARTICLE{firth57synopsis,
    abstract = {Reprinted in:  Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London. },
    added-at = {2008-05-14T00:52:58.000+0200},
    address = {Oxford},
    author = {Firth, J. R.},
    biburl = {https://www.bibsonomy.org/bibtex/25e3d6c72cdd123a638f71886d78f3c1e/brightbyte},
    booktitle = {Studies in Linguistic Analysis (special volume of the Philological Society)},
    interhash = {b4f769667fdd195b4a75f61f6388a52e},
    intrahash = {5e3d6c72cdd123a638f71886d78f3c1e},
    keywords = {classic linguistics meanign relatedness semantic},
    pages = {1-32},
    publisher = {The Philological Society},
    timestamp = {2009-01-23T09:58:50.000+0100},
    title = {A synopsis of linguistic theory 1930-55.},
    volume = {1952-59},
    year = 1957
}

@INPROCEEDINGS{Hofmann2021SuperbizarreIN,
    title={Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words},
    author={Valentin Hofmann and Janet B. Pierrehumbert and Hinrich Sch{\"u}tze},
    booktitle={ACL/IJCNLP},
    year={2021}
}

@INPROCEEDINGS{Virpioja2013Morfessor2P,
    title={Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline},
    author={Sami Virpioja and Peter Smit and Stig-Arne Gr{\"o}nroos and Mikko Kurimo},
    year={2013}
}

@INPROCEEDINGS{Creutz04inductionof,
    author = {Mathias Creutz and Krista Lagus},
    title = {Induction of a simple morphology for highly-inflecting languages},
    booktitle = {IN PROCEEDINGS OF THE 7TH MEETING OF THE ACL SPECIAL INTEREST GROUP IN COMPUTATIONAL PHONOLOGY (SIGPHON},
    year = {2004},
    pages = {43--51},
    publisher = {}
}

@INPROCEEDINGS{Creutz05inducingthe,
    author = {Mathias Creutz and Krista Lagus},
    title = {Inducing the Morphological Lexicon of a Natural Language from Unannotated Text},
    booktitle = {In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR’05},
    year = {2005},
    pages = {106--113}
}

@INPROCEEDINGS{Creutz05unsupervisedmorpheme,
    author = {Mathias Creutz and Krista Lagus},
    title = {Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0},
    booktitle = {Helsinki University of Technology},
    year = {2005}
}


@INPROCEEDINGS{fonseca2016assin,
  title={ASSIN: Avaliacao de similaridade semantica e inferencia textual},
  author={Fonseca, E and Santos, L and Criscuolo, Marcelo and Aluisio, S},
  booktitle={Computational Processing of the Portuguese Language-12th International Conference, Tomar, Portugal},
  pages={13--15},
  year={2016}
}


@INCOLLECTION{Rocha2017Aug,
	author = {Rocha, Gil and Cardoso, Henrique Lopes},
	title = {{Recognizing Textual Entailment and Paraphrases in Portuguese}},
	booktitle = {{Progress in Artificial Intelligence}},
	journal = {SpringerLink},
	pages = {868--879},
	year = {2017},
	month = {Aug},
	isbn = {978-3-319-65339-6},
	publisher = {Springer},
	address = {Cham, Switzerland},
	doi = {10.1007/978-3-319-65340-2_70}
}


@ARTICLE{creutz-lagus-2007-unsupervised,
    author = {Creutz, Mathias and Lagus, Krista},
    year = {2007},
    month = {01},
    pages = {},
    title = {Unsupervised models for morpheme segmentation and morphology learning},
    volume = {4},
    journal = {TSLP},
    doi = {10.1145/1217098.1217101}
}

@ARTICLE{morphology-matters-tacl-2021,
    author = {Park, Hyunji Hayley and Zhang, Katherine J. and Haley, Coleman and Steimel, Kenneth and Liu, Han and Schwartz, Lane},
    title = "{Morphology Matters: A Multilingual Language Modeling Analysis}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {261-276},
    year = {2021},
    month = {03},
    abstract = "{Prior studies in multilingual language modeling (e.g., Cotterell et al., 2018; Mielke et al., 2019) disagree on whether or not inflectional morphology makes languages harder to model. We attempt to resolve the disagreement and extend those studies. We compile a larger corpus of 145 Bible translations in 92 languages and a larger number of typological features.1 We fill in missing typological data for several languages and consider corpus-based measures of morphological complexity in addition to expert-produced typological features. We find that several morphological measures are significantly associated with higher surprisal when LSTM models are trained with BPE-segmented data. We also investigate linguistically motivated subword segmentation strategies like Morfessor and Finite-State Transducers (FSTs) and find that these segmentation strategies yield better performance and reduce the impact of a language’s morphology on language modeling.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00365},
    url = {https://doi.org/10.1162/tacl\_a\_00365},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00365/1924158/tacl\_a\_00365.pdf},
}

@INPROCEEDINGS{a-neural-probabilistic-lm,
author = {Bengio, Y. and Ducharme, Réjean and Vincent, Pascal},
year = {2000},
month = {01},
pages = {932-938},
title = {A Neural Probabilistic Language Model},
volume = {3},
journal = {Journal of Machine Learning Research},
doi = {10.1162/153244303322533223}
}

@INPROCEEDINGS{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@INPROCEEDINGS{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}


@INPROCEEDINGS{
    micikevicius2018mixed,
    title={Mixed Precision Training},
    author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=r1gs9JgRZ},
}


@INPROCEEDINGS{
    loshchilov2018decoupled,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@INPROCEEDINGS{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}


@article{kohavi2001,
author = {Kohavi, Ron},
year = {2001},
month = {03},
pages = {},
title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
volume = {14}
}


@ARTICLE{Taylor1953ClozePA,
  title={“Cloze Procedure”: A New Tool for Measuring Readability},
  author={Wilson L. Taylor},
  journal={Journalism \& Mass Communication Quarterly},
  year={1953},
  volume={30},
  pages={415 - 433}
}


@MISC{morph-typology,
    author = {{Jonathan Manker}},
    title = {Morphological Typology},
    howpublished = {https://linguistics.berkeley.edu/~jtmanker/Morphological\%20Typology\%20-\%20Spring\%202016\%20-\%20Ling\%20100\%20Guest\%20Lecture.pdf},
    year = {2016},
    note = "[Online; acessado em 16 de Fevereiro de 2022]"
}

@MISC{quentin_lhoest_2021_5639822,
  author       = {Quentin Lhoest and
                  Albert Villanova del Moral and
                  Patrick von Platen and
                  Thomas Wolf and
                  Mario Šaško and
                  Yacine Jernite and
                  Abhishek Thakur and
                  Lewis Tunstall and
                  Suraj Patil and
                  Mariama Drame and
                  Julien Chaumond and
                  Julien Plu and
                  Joe Davison and
                  Simon Brandeis and
                  Victor Sanh and
                  Teven Le Scao and
                  Kevin Canwen Xu and
                  Nicolas Patry and
                  Steven Liu and
                  Angelina McMillan-Major and
                  Philipp Schmid and
                  Sylvain Gugger and
                  Nathan Raw and
                  Sylvain Lesage and
                  Anton Lozhkov and
                  Matthew Carrigan and
                  Théo Matussière and
                  Leandro von Werra and
                  Lysandre Debut and
                  Stas Bekman and
                  Clément Delangue},
  title        = {huggingface/datasets: 1.15.1},
  month        = nov,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.15.1},
  doi          = {10.5281/zenodo.5639822},
  url          = {https://doi.org/10.5281/zenodo.5639822}
}

@MISC{folha-dataset,
  author = {Marlesson Santana},
  title = {{news of the site Folha de São Paulo (Brazilian Newspaper), Version 2}},
  howpublished = "\url{https://www.kaggle.com/datasets/marlesson/news-of-the-site-folhauol}",
  year = {2019}, 
  note = "[Online; acessado em 24 de Novembro de 2021]"
}


@misc{Gomes2020,
  author = {GOMES, J. R. S.},
  title = {PLUE: Portuguese Language Understanding Evaluation},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jubs12/PLUE}},
  commit = {e7d01cb17173fe54deddd421dd735920964eb26f}
}


@INPROCEEDINGS{fakebr:18,
author={Monteiro, Rafael A. and Santos, Roney L. S. and Pardo, Thiago A. S. and de Almeida, Tiago A. and Ruiz, Evandro E. S. and Vale, Oto A.},
title={Contributions to the Study of Fake News in Portuguese: New Corpus and Automatic Detection Results},
booktitle={Computational Processing of the Portuguese Language},
year={2018},
publisher={Springer International Publishing},
pages={324--334},
isbn={978-3-319-99722-3},
}

@INPROCEEDINGS{matsakis2014rust,
  title={The rust language},
  author={Matsakis, Nicholas D and Klock II, Felix S},
  booktitle={ACM SIGAda Ada Letters},
  volume={34},
  number={3},
  pages={103--104},
  year={2014},
  organization={ACM}
}

@ARTICLE{Gage1994Feb,
	author = {Gage, Philip},
	title = {{A new algorithm for data compression}},
	journal = {C Users J.},
	volume = {12},
	number = {2},
	pages = {23--38},
	year = {1994},
	month = {Feb},
	issn = {0898-9788},
	publisher = {R {\&} D Publications, Inc.},
	doi = {10.5555/177910.177914}
}

@ARTICLE{silva:20,
title = "Towards automatically filtering fake news in Portuguese",
journal = "Expert Systems with Applications",
volume = "146",
pages = "113199",
year = "2020",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2020.113199",
url = "http://www.sciencedirect.com/science/article/pii/S0957417420300257",
author = "Renato M. Silva and Roney L.S. Santos and Tiago A. Almeida and Thiago A.S. Pardo",
}


@ARTICLE{cotterell-schutze-2018-joint,
    title = "Joint Semantic Synthesis and Morphological Analysis of the Derived Word",
    author = {Cotterell, Ryan  and
      Sch{\"u}tze, Hinrich},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1003",
    doi = "10.1162/tacl_a_00003",
    pages = "33--48",
    abstract = "Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word{'}s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3{\%} and 5{\%}. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist{'}s notion of morphological productivity.",
}


@INPROCEEDINGS{cotterell-etal-2016-morphological-segmentation,
    title = "Morphological Segmentation Inside-Out",
    author = {Cotterell, Ryan  and
      Kumar, Arun  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1256",
    doi = "10.18653/v1/D16-1256",
    pages = "2325--2330",
}


@INPROCEEDINGS{cotterell-etal-2016-joint,
    title = "A Joint Model of Orthography and Morphological Segmentation",
    author = {Cotterell, Ryan  and
      Vieira, Tim  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1080",
    doi = "10.18653/v1/N16-1080",
    pages = "664--669",
}

@INPROCEEDINGS{conneau-etal-2018-cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
    abstract = "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. {``}Downstream{''} tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",
}


@INPROCEEDINGS{Hewitt2019ASP,
  title={A Structural Probe for Finding Syntax in Word Representations},
  author={John Hewitt and Christopher D. Manning},
  booktitle={NAACL},
  year={2019}
}


@INPROCEEDINGS{chen2021probing,
    title={Probing {BERT} in Hyperbolic Spaces},
    author={Boli Chen and Yao Fu and Guangwei Xu and Pengjun Xie and Chuanqi Tan and Mosha Chen and Liping Jing},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=17VnwXYZyhH}
}


@INPROCEEDINGS{Sundermeyer2012LSTMNN,
  title={LSTM Neural Networks for Language Modeling},
  author={Martin Sundermeyer and Ralf Schl{\"u}ter and Hermann Ney},
  booktitle={INTERSPEECH},
  year={2012}
}


@MISC{Al-Rfou2020,
  author = {Al-Rfou, Rami},
  title = {Polyglot},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/aboSamoor/polyglot}},
  commit = {9b93b2ecbb9ba1f638c56b92665336e93230646a}
}

@MISC{anthony_moi_2022_5862127,
  author       = {Anthony MOI and
                  Pierric Cistac and
                  Nicolas Patry and
                  Pete and
                  Funtowicz Morgan and
                  Sebastian Pütz and
                  Bjarte Johansen and
                  Thomas Wolf and
                  Sylvain Gugger and
                  Clement and
                  Julien Chaumond and
                  Lysandre Debut and
                  François Garillot and
                  Luc Georges and
                  Taufiquzzaman Peyash and
                  0xflotus and
                  Alan deLevie and
                  Alexander Mamaev and
                  Colin Clement and
                  Dagmawi Moges and
                  Denis Zolotukhin and
                  Geoffrey Thomas and
                  Ivan Echevarria and
                  JC Louis and
                  Karan Desai and
                  Koichi Yasuoka and
                  MarcusGrass and
                  Mishig Davaadorj and
                  Mohamed Al Salti},
  title        = {huggingface/tokenizers: Python v0.11.3},
  month        = jan,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {python-v0.11.3},
  doi          = {10.5281/zenodo.5862127},
  url          = {https://doi.org/10.5281/zenodo.5862127}
}

@MISC{thomas_wolf_2021_5045610,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Sylvain Gugger and
                  Patrick von Platen and
                  Julien Chaumond and
                  Stas Bekman and
                  Sam Shleifer and
                  Victor SANH and
                  Manuel Romero and
                  Funtowicz Morgan and
                  Julien Plu and
                  Suraj Patil and
                  Aymeric Augustin and
                  Rémi Louf and
                  Stefan Schweter and
                  Denis and
                  erenup and
                  Matt and
                  Nicolas Patry and
                  Joe Davison and
                  Anthony MOI and
                  Philipp Schmid and
                  Teven and
                  Piero Molino and
                  Grégory Châtel and
                  Bram Vanroy and
                  Clement and
                  Kevin Canwen Xu and
                  Daniel Stancl and
                  Philip May},
  title        = {huggingface/transformers:},
  month        = jun,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v4},
  doi          = {10.5281/zenodo.5045610},
  url          = {https://doi.org/10.5281/zenodo.5045610}
}

@INPROCEEDINGS{souza2020,
author="Souza, F{\'a}bio
and Nogueira, Rodrigo
and Lotufo, Roberto",
editor="Cerri, Ricardo
and Prati, Ronaldo C.",
title="BERTimbau: Pretrained BERT Models for Brazilian Portuguese",
booktitle="Intelligent Systems",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="403--417",
abstract="Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of large pretrained language models (LMs) to downstream natural language processing (NLP) tasks. This transfer learning approach improves the overall performance on many tasks and is highly beneficial when labeled data is scarce, making pretrained LMs valuable resources specially for languages with few annotated training examples. In this work, we train BERT (Bidirectional Encoder Representations from Transformers) models for Brazilian Portuguese, which we nickname BERTimbau. We evaluate our models on three downstream NLP tasks: sentence textual similarity, recognizing textual entailment, and named entity recognition. Our models improve the state-of-the-art in all of these tasks, outperforming Multilingual BERT and confirming the effectiveness of large pretrained LMs for Portuguese. We release our models to the community hoping to provide strong baselines for future NLP research: https://github.com/neuralmind-ai/portuguese-bert.",
isbn="978-3-030-61377-8"
}


@INBOOK{Bisong2019,
    author="Bisong, Ekaba",
    title="Google Colaboratory",
    bookTitle="Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners",
    year="2019",
    publisher="Apress",
    address="Berkeley, CA",
    pages="59--64",
    abstract="Google Colaboratory more commonly referred to as ``Google Colab'' or just simply ``Colab'' is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.",
    isbn="978-1-4842-4470-8",
    doi="10.1007/978-1-4842-4470-8_7",
    url="https://doi.org/10.1007/978-1-4842-4470-8_7"
}


@BOOK{VanRossum2009,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}


@INPROCEEDINGS{brown2020,
    title = {Language {Models} are {Few}-{Shot} {Learners}},
    volume = {33},
    url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates, Inc.},
    author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
    year = {2020},
    pages = {1877--1901},
}


@INPROCEEDINGS{Adams2009Conceptual,
  author = {Adams, B. and Raubal, M.},
  title = {Conceptual Space Markup Language (CSML): Towards the Cognitive Semantic Web},
  booktitle = {Proceedings...},
  year = {2009},
  organization = {IEEE International Conference on Semantic Computing},
  conference-year = {2009},
  conference-location = {Berkeley, USA},
  pages = {253–260},
  address = {Washington, USA},
  month = sep,
  publisher = {IEEE},
  ab-stractnote = {CSML is a semantic markup language created for the publishing and
	sharing of conceptual spaces, which are geometric structures that
	represent semantics at the conceptual level. CSML can be used to
	describe semantics that are not captured well by the ontology languages
	commonly used in the Semantic Web. Measurement of the semantic similarity
	of concepts as well as the combination of concepts without shared
	properties are common human cognitive tasks. However, these operations
	present sources of difficulty for tools reliant upon set-theoretic
	and syllogistic reasoning on symbolic ontologies. In contrast, these
	operations can be modeled naturally using conceptual spaces. This
	paper describes the design decisions behind CSML, introduces the
	key component elements of a CSML document, and presents examples
	of its usage.},
  doi = {10.1109/ICSC.2009.58},
  owner = {srfiorini},
  timestamp = {2013.07.19}
}

@INBOOK{parquet,
author = {Vohra, Deepak},
year = {2016},
month = {09},
pages = {325-335},
title = {Apache Parquet},
isbn = {978-1-4842-2198-3},
doi = {10.1007/978-1-4842-2199-0_8}
}


@comment{jabref-meta: selector_keywords:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_review:}